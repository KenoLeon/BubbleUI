{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNi35Epl/+G+k4Of0OXOa9Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Install Unsloth and dependencies (Colab) ---\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "4kAQVGcSCENN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig4R7OlXBtY6"
      },
      "outputs": [],
      "source": [
        "# --- Load Model and Tokenizer ---\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# --- Prepare for Inference ---\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\",\n",
        "    mapping = {\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Enable fast inference\n",
        "\n",
        "# # --- Inference Function ---\n",
        "# def get_llm_response(prompt, max_new_tokens=128):\n",
        "#     messages = [{\"from\": \"human\", \"value\": prompt}]\n",
        "#     inputs = tokenizer.apply_chat_template(\n",
        "#         messages,\n",
        "#         tokenize=True,\n",
        "#         add_generation_prompt=True,\n",
        "#         return_tensors=\"pt\"\n",
        "#     ).to(\"cuda\")\n",
        "#     outputs = model.generate(input_ids=inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
        "#     response = tokenizer.batch_decode(outputs)[0]\n",
        "#     return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(prompt, max_new_tokens=128):\n",
        "    messages = [{\"from\": \"human\", \"value\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "    outputs = model.generate(input_ids=inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    # Extract only the assistant's reply\n",
        "    if \"<|assistant|>\" in response:\n",
        "        reply = response.split(\"<|assistant|>\")[-1]\n",
        "        # Remove any trailing special tokens\n",
        "        reply = reply.split(\"<|end|>\")[0].strip()\n",
        "        return reply\n",
        "    else:\n",
        "        return response.strip()"
      ],
      "metadata": {
        "id": "VMepm0g7PGSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage ---\n",
        "print(get_llm_response(\"Whats are some popular dog breeds ?\"))"
      ],
      "metadata": {
        "id": "E7GwQVNiCrsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask pyngrok flask-cors\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "\n",
        "# Set your ngrok authtoken (replace with your actual token)\n",
        "ngrok.set_auth_token(\"2tpeLhQarUCoqaTW6vb2nGNCtza_4RTPRqBnKy4V74pQ9v53r\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for all origins\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "    prompt = data.get(\"prompt\", \"\")\n",
        "    response = get_llm_response(prompt)\n",
        "    return jsonify({\"response\": response})\n",
        "\n",
        "def run():\n",
        "    app.run(port=7070)\n",
        "Thread(target=run).start()\n",
        "\n",
        "public_url = ngrok.connect(7070, bind_tls=True, domain=\"tightly-fit-tetra.ngrok-free.app\")\n",
        "print(\" * ngrok tunnel:\", public_url)"
      ],
      "metadata": {
        "id": "Fy7kE3pMNRLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Kill all ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Kill any Flask servers (optional, but helps)\n",
        "os.system(\"pkill -f flask\")"
      ],
      "metadata": {
        "id": "LTx9Ek2aS6wV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}